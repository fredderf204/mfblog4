<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Using Python to look at how to increase performance and reduce per call latency of Azure Open AI."><meta name=keywords content="Azure Open AI,Python,Performance,Latency"><title>Azure Open AI - Increase Performance and reduce per call latency</title>
<link rel=canonical href=https://mfblog.au/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Azure Open AI - Increase Performance and reduce per call latency"><meta property='og:description' content="Using Python to look at how to increase performance and reduce per call latency of Azure Open AI."><meta property='og:url' content='https://mfblog.au/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/'><meta property='og:site_name' content='MF Azure Blog'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Azure Open AI'><meta property='article:published_time' content='2024-01-25T11:39:17+11:00'><meta property='article:modified_time' content='2024-01-25T11:39:17+11:00'><meta property='og:image' content='https://mfblog.au/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/cover.jpg'><meta name=twitter:title content="Azure Open AI - Increase Performance and reduce per call latency"><meta name=twitter:description content="Using Python to look at how to increase performance and reduce per call latency of Azure Open AI."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://mfblog.au/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/cover.jpg'><link rel="shortcut icon" href=/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-C4H16BVKJJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-C4H16BVKJJ")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/me_hu8008747913738786229.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>☁️</span></figure><div class=site-meta><h1 class=site-name><a href=/>MF Azure Blog</a></h1><h2 class=site-description>Short musings found while working with Azure | Opinions are my own</h2></div></header><ol class=menu-social><li><a href=https://github.com/fredderf204 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/1michaelfriedrich/ target=_blank title=Linkedin rel=me><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg></a></li><li><a href=https://twitter.com/fredderf204 target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#tldr>TLDR</a></li><li><a href=#streaming>Streaming</a></li><li><a href=#testing-setup>Testing setup</a></li><li><a href=#another-way-to-monitor-per-call-latency>Another way to monitor per call latency</a></li><li><a href=#test-1---use-the-latest-models-in-the-gpt-35-turbo-model-series>Test 1 - Use the latest models in the GPT-3.5 Turbo model series</a></li><li><a href=#test-2---reduce-the-amount-of-completion-tokens-in-the-model-response>Test 2 - Reduce the amount of completion tokens in the model response</a></li><li><a href=#test-3---use-the-max_tokens-and-stop-parameters>Test 3 - Use the max_tokens and stop parameters</a></li><li><a href=#large-prompt-tokens-have-little-impact-on-performance-and-per-call-latency>Large prompt tokens have little impact on performance and per call latency</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/><img src=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/cover_hu2025744306185113670.jpg srcset="/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/cover_hu2025744306185113670.jpg 800w, /p/azure-open-ai-increase-performance-and-reduce-per-call-latency/cover_hu2964685437403798109.jpg 1600w" width=800 height=517 loading=lazy alt="Featured image of post Azure Open AI - Increase Performance and reduce per call latency"></a></div><div class=article-details><header class=article-category><a href=/categories/azure/>Azure
</a><a href=/categories/ai/>AI
</a><a href=/categories/python/>Python</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/>Azure Open AI - Increase Performance and reduce per call latency</a></h2><h3 class=article-subtitle>Using Python to look at how to increase performance and reduce per call latency of Azure Open AI.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jan 25, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>11 minute read</time></div></footer></div></header><section class=article-content><blockquote><p>I am a Microsoft employee, but the views expressed here are mine and not those of my employer.</p></blockquote><p>I have been working with some clients that are new to Azure Open AI and they have been asking me about how to increase performance and reduce per call latency within the service. Usually I would talk them through <a class=link href=https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency target=_blank rel=noopener>this</a> Microsoft article. But I thought I would go one step further with this article and use Python and test calls demonstrate the performance and latency of Azure Open AI.</p><h2 id=tldr>TLDR</h2><p>If the performance and per call latency in Azure Open AI is not meeting you expectations, then make these changes, if possible in this order</p><ul><li>If you are creating a Chat bot or chat experience, enable stream mode in the call.</li><li>Use the latest models in the <a class=link href=https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35-models target=_blank rel=noopener>GPT-3.5 Turbo model series</a>.</li><li>Reduce the amount of completion tokens in the model response.</li><li>Use the max_tokens and stop parameters in the call.</li></ul><h2 id=streaming>Streaming</h2><p>The below is taken from <a class=link href=https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency#streaming target=_blank rel=noopener>https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency#streaming</a>;</p><p>&ldquo;Setting <code>stream: true</code> in a request makes the service return tokens as soon as they&rsquo;re available, instead of waiting for the full sequence of tokens to be generated. It doesn&rsquo;t change the time to get all the tokens, but it reduces the time for first response. This approach provides a better user experience since end-users can read the response as it is generated.&rdquo;</p><p>I didn&rsquo;t think there was a need to test this. As I think the premise is sound and there are lots of examples where this provides a great user experience. I.e. ChatGPT.</p><h2 id=testing-setup>Testing setup</h2><p>The other 3 items in the TLDR we are going to test.</p><p>We will be completing all of the test against Azure Open AI completions endpoint. All of the test were conducted with the following;</p><ol><li>All tests were run on a VM deployed into the Australia East region.</li><li>The Azure Open AI service was deployed to the Australia East region.</li><li>The code that was used to create the tests can be found <a class=link href=https://github.com/fredderf204/aoai-tester target=_blank rel=noopener>here</a>.</li><li>The Python code was run as part of a cronjob on the VM. I modified the crontab file and add a new line like this; <code>* * * * * /usr/bin/python3 /home/fredderf204/aoai-tester/app.py >> /home/fredderf204/aoai-tester/aoai-tester.log 2>&amp;1</code>. This means that the test will run every 1 minute and the output (both STDOUT and STDERR) will be written to a log file.</li><li>I collected the data from the log file and put it into a spreadsheet.</li></ol><h2 id=another-way-to-monitor-per-call-latency>Another way to monitor per call latency</h2><p>I will be writing the results of my python code to a log file on the VM. I did this because I thought it would be a good way to record and measure the results.</p><p>But there is another way to monitor per call latency, and it&rsquo;s with Azure Monitor. <a class=link href=https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring target=_blank rel=noopener>This</a> article goes into detail about how to do this. And the TLDR is that you can simply run your code against Azure Open AI and see the per call latency by clicking &ldquo;Metrics&rdquo; in the Azure Portal under the Monitoring section. Then select the latency metric and you will see something like this;</p><p><img src=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/1.jpg width=266 height=302 srcset="/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/1_hu2192434948362282070.jpg 480w, /p/azure-open-ai-increase-performance-and-reduce-per-call-latency/1_hu4557020825789625728.jpg 1024w" loading=lazy alt="Per call latency" class=gallery-image data-flex-grow=88 data-flex-basis=211px></p><p>You can further filter by deployment, or api version, or endpoint. But to be honest, the only reason I didn&rsquo;t do this was because I didn&rsquo;t know how to get the data into excel to create the graphs. But I am sure there is a way.</p><h2 id=test-1---use-the-latest-models-in-the-gpt-35-turbo-model-series>Test 1 - Use the latest models in the GPT-3.5 Turbo model series</h2><p>The first test will be a per call latency test comparing GPT-4 vs GPT-3.5 Turbo. The general advice I give to my clients is the try GPT-3.5 Turbo model series to see if it meets your needs. If it does not, there try out GPT-4 model series. You will see below that GPT-3.5 Turbo is more performant and has a lower per call latency compared to GPT-4. But GPT-4 is more accurate, so it&rsquo;s a trade off.</p><p>In the below chart I sent 20 requests to each model and recorded the results. As mentioned previously, I used the code in this repo <a class=link href=https://github.com/fredderf204/aoai-tester target=_blank rel=noopener>here</a> and used this prompt <code>write a 500 word short story</code>. I chose this prompt for 2 reasons;</p><ol><li>It&rsquo;s simple</li><li>It&rsquo;s going to create a fairly large amount completion tokens.</li></ol><p><img src=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/2.jpg width=481 height=289 srcset="/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/2_hu9499447220558582891.jpg 480w, /p/azure-open-ai-increase-performance-and-reduce-per-call-latency/2_hu3940726821940932442.jpg 1024w" loading=lazy alt="GPT-4 vs GPT-3.5 Turbo" class=gallery-image data-flex-grow=166 data-flex-basis=399px></p><p>GPT-4 had an average end to end latency of 53.13 seconds and generated an average of 687.55 completion tokens per call.</p><p>GPT-3.5 Turbo had an average end to end latency of 7.77 seconds and generated an average of 756.9 completion tokens per call.</p><blockquote><p>As you can see GPT-3.5 Turbo is an order of magnitude faster than GPT-4. If you can switch from GPT-4 to GPT-3.5 Turbo, then you will see a quite an improvement in performance and per call latency. Which is why this is the first recommendation I make to my clients.</p></blockquote><h2 id=test-2---reduce-the-amount-of-completion-tokens-in-the-model-response>Test 2 - Reduce the amount of completion tokens in the model response</h2><p>For this test, we will take the previous results from GPT-4 and the prompt <code>write a 500 word short story</code> and compare it to the same model but use a modified prompt <code>write a 300 word short story</code>. The only difference between the 2 prompts is the number of words in the story. Which should result in fewer completion tokens being generated.</p><p>Again we sent 20 calls to the service and recorded the results.</p><p><img src=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/3.jpg width=482 height=287 srcset="/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/3_hu11986053848499795972.jpg 480w, /p/azure-open-ai-increase-performance-and-reduce-per-call-latency/3_hu17588050302438419109.jpg 1024w" loading=lazy alt="GPT-4 - 500 vs 300 words" class=gallery-image data-flex-grow=167 data-flex-basis=403px></p><p>Prompt 1 (500 word short story) resulted in an average end to end latency of 53.13 seconds and generated an average of 687.55 completion tokens per call.</p><p>Prompt 2 (300 word short story) resulted in an average end to end latency of 25.33 seconds and generated an average of 434.05 completion tokens per call.</p><blockquote><p>As you can see, there is a direct correlation between the amount of completion tokens and the end to end latency. The more completion tokens, the longer the end to end latency. So if you can reduce the amount of completion tokens, you will see a reduction in end to end latency.</p></blockquote><p><a class=link href=https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency#generation-size-and-max-tokens target=_blank rel=noopener>This</a> article goes into more detail about why this is.</p><h2 id=test-3---use-the-max_tokens-and-stop-parameters>Test 3 - Use the max_tokens and stop parameters</h2><p>For this test, one set of requests will have the max_tokens and stop parameters set and the other will not. We will use a new prompt which is larger and uses 2 shot examples. Both prompts were run against GPT-4. Please see the prompt below;</p><p><code>write a 300 word story that begins with Once upon a time and ends with The End \n\nExample 1: Once upon a time, in a faraway land, there lived a young girl named Lila. She was a curious and adventurous girl who loved to explore the world around her. One day, while wandering through the woods, she stumbled upon a hidden cave. The entrance was small, but Lila was determined to explore it. She squeezed through the narrow opening and found herself in a large cavern. The walls were covered in glittering crystals that sparkled in the dim light. Lila was amazed by the beauty of the cave and decided to explore further.\n\nAs she walked deeper into the cave, she noticed a faint glow in the distance. She followed the light and soon found herself in a large chamber. In the center of the room was a glowing orb that pulsed with a soft light. Lila approached the orb and reached out to touch it. Suddenly, she was enveloped in a bright light and felt herself being lifted off the ground.\n\nWhen the light faded, Lila found herself in a strange new world. The sky was a deep shade of purple, and the trees were made of crystal. She looked around in wonder and realized that she had been transported to a magical realm.\n\nLila spent many years exploring this new world, meeting strange creatures and having incredible adventures. But eventually, she grew homesick and decided to return to her own world. She approached the glowing orb once again and was enveloped in a bright light. When the light faded, she found herself back in the cave where she had started.\n\nLila emerged from the cave, changed forever by her incredible journey. She went on to live a long and happy life, but she never forgot the magic of that other world. And so, we come to the end of our story. The End.\n\nExample 2: Once upon a time, in the kingdom of Uriel, lived an old shoemaker named Bartolo. He was a quiet man who lived a simple life, crafting the most beautiful shoes to earn his livelihood. But Bartolo had a secret - he could weave magic into his shoes, giving them abilities far beyond any regular footwear. And so, Uriel\'s inhabitants adored his creations, no one knowing Bartolo was actually a wizard in disguise.\n\nOne gloomy evening, Bartolo noticed a young girl sobbing outside his shop. Her cloth shoes were ripped and worn, her feet bloodied from the rough city cobblestones. Bartolo\'s heart squeezed at the sight. He called out to her, promising a pair of new shoes.\n\nThe next day, Bartolo presented the girl, Lily, with enchanted dancing shoes. They would protect her feet and lead her heart to happiness. She looked up at Bartolo, her eyes wet with gratitude, and danced away, leaving a trail of soft laughter and twinkling sparkles.\n\nWeeks later, a royal proclamation announced a grand ball to choose a prince\'s bride, and Lily decided to attend. That night, her gifted shoes guided her as she danced with a grace that awoke an otherworldly charm. The prince was instantly smitten by Lily\'s uniqueness and chose her as his bride.\n\nOn their wedding, Bartolo presented a wondrous pair of shoes to Lily as a wedding gift - shoes embedded with everlasting love and compassion. Witnessing Lily\'s happiness, Bartolo felt his life had been worthwhile. He had used his magic to give joy, and in return, he was filled with contentment.\n\nYears later, after living a fulfilled magical life, Bartolo passed away. But through Lily, his magical shoes, and the little happiness they brought to this kingdom, his legacy lived on, forever woven into the fabric of Uriel\'s tales. Thus, our story concludes, until we meet again under the moonlit pages of another tale. The End.</code></p><p>For the second set of calls, we will set the max_token parameter to 450 and stop to <code>The End</code>. This means that the model will stop generating tokens 3 ways;</p><ol><li>When it finishes generating the response</li><li>When it hits the stop sequence of <code>The End</code>.</li><li>When it reaches 450 tokens completion tokens.</li></ol><p>This should result in fewer completion tokens being generated.</p><p><img src=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/4.jpg width=482 height=289 srcset="/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/4_hu3676537769972848429.jpg 480w, /p/azure-open-ai-increase-performance-and-reduce-per-call-latency/4_hu4820296070055679735.jpg 1024w" loading=lazy alt="Max tokens and stop" class=gallery-image data-flex-grow=166 data-flex-basis=400px></p><p>Call set 1 (no max_tokens or stop) resulted in an average end to end latency of 38.48 seconds and generated an average of 438.35 completion tokens per call.</p><p>Call set 2 (max_tokens = 450 and stop = The End) resulted in an average end to end latency of 34.55 seconds and generated an average of 426.8 completion tokens per call.</p><blockquote><p>I know it a little hard to tell in the chart, but there was a slight improvement in end to end latency and a slight reduction in the amount of completion tokens. This is why I would recommend this as the last thing to try. As it&rsquo;s not going to have a huge impact on performance or per call latency in comparison to the other 2 items above.</p></blockquote><h2 id=large-prompt-tokens-have-little-impact-on-performance-and-per-call-latency>Large prompt tokens have little impact on performance and per call latency</h2><p>Going into this experiment, I though the amount of prompt tokens would have a large impact on performance and per call latency. Turns out that is not the case! I did 1 final test. Against GPT-4 I used our original prompt of <code>write a 500 word short story</code> and ran a test set. Then I used another prompt that started out the same, and I added a random set of 5000 words to the end of the prompt.</p><p>This resulted in first prompt having 14 prompt tokens and the second prompt having on average 7091 prompt tokens. I ran 20 calls against each prompt and recorded the results.</p><p><img src=/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/5.jpg width=481 height=289 srcset="/p/azure-open-ai-increase-performance-and-reduce-per-call-latency/5_hu2548036513861351607.jpg 480w, /p/azure-open-ai-increase-performance-and-reduce-per-call-latency/5_hu13399945361136962344.jpg 1024w" loading=lazy alt="Large prompt tokens" class=gallery-image data-flex-grow=166 data-flex-basis=399px></p><p>Call set 1 (14 prompt tokens) resulted in an average end to end latency of 53.13 seconds and generated an average of 687.55 completion tokens per call. I used the same data as the latest models test.</p><p>Call set 2 (7091 prompt tokens) resulted in an average end to end latency of 53.47 seconds and generated an average of 813.1 completion tokens per call.</p><blockquote><p>There seems to be almost no difference in performance or per call latency when using a large amount of prompt tokens. So if you need to use a large amount of prompt tokens, then you should be ok.</p></blockquote><h2 id=conclusion>Conclusion</h2><p>I would reccomend everyone go and read <a class=link href=https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency target=_blank rel=noopener>https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency</a> as it has some great information about how to increase performance and reduce per call latency in Azure Open AI. Which we have just proved with Python in our simple experiments.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/azure-open-ai/>Azure Open AI</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/part-2-simple-gen-ai-product-recommendation-system/><div class=article-image><img src=/p/part-2-simple-gen-ai-product-recommendation-system/cover.d40a3a4a6b22a2f44f45527adcfee6bd_hu17088848604811627793.jpg width=250 height=150 loading=lazy alt="Featured image of post Part 2 - Simple Gen AI Product Recommendation System" data-hash="md5-1Ao6SmsiovRPRVJ63P7mvQ=="></div><div class=article-details><h2 class=article-title>Part 2 - Simple Gen AI Product Recommendation System</h2></div></a></article><article class=has-image><a href=/p/simple-gen-ai-product-recommendation-system-you-can-poc-quickly/><div class=article-image><img src=/p/simple-gen-ai-product-recommendation-system-you-can-poc-quickly/cover.bc1a769b2bf354d8a87d51397f8da7fc_hu16783130262988169499.jpg width=250 height=150 loading=lazy alt="Featured image of post Simple Gen AI Product Recommendation System you can POC quickly" data-hash="md5-vBp2myvzVNiofVE5f42n/A=="></div><div class=article-details><h2 class=article-title>Simple Gen AI Product Recommendation System you can POC quickly</h2></div></a></article><article class=has-image><a href=/p/automated-document-processing-and-fraud-detection-with-azure/><div class=article-image><img src=/p/automated-document-processing-and-fraud-detection-with-azure/cover.8031a5ff034bbf5b0b3d687df799c87e_hu3040367034163266224.jpg width=250 height=150 loading=lazy alt="Featured image of post Automated document processing and fraud detection with Azure" data-hash="md5-gDGl/wNLv1sLPWh995nIfg=="></div><div class=article-details><h2 class=article-title>Automated document processing and fraud detection with Azure</h2></div></a></article><article class=has-image><a href=/p/ai-generated-azure-news-summary/><div class=article-image><img src=/p/ai-generated-azure-news-summary/cover.64db9f55634ff34eaf2d4d2cb156527c_hu3456839047031088951.jpg width=250 height=150 loading=lazy alt="Featured image of post AI Generated Azure News Summary" data-hash="md5-ZNufVWNP806vLU0ssVZSfA=="></div><div class=article-details><h2 class=article-title>AI Generated Azure News Summary</h2></div></a></article><article class=has-image><a href=/p/using-ssml-with-azure-ai-speech-service/><div class=article-image><img src=/p/using-ssml-with-azure-ai-speech-service/cover.dff5bfee24dd220ed43e30f71e5d8ff0_hu6794696554352319624.jpeg width=250 height=150 loading=lazy alt="Featured image of post Using SSML with Azure AI Speech service" data-hash="md5-3/W/7iTdIg7UPjD3Hl2P8A=="></div><div class=article-details><h2 class=article-title>Using SSML with Azure AI Speech service</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=fredderf204/mfblog4-giscus data-repo-id=R_kgDOK5HFPw data-category=Announcements data-category-id=DIC_kwDOK5HFP84Cbspg data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark")}})()</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 MF Azure Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>